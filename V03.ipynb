{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import genfromtxt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SOM(object):\n",
    "    def __init__(self, m, n):\n",
    "        self.net_dim = np.array([m, n])\n",
    "        self.weights = None\n",
    "        self.hit_map = np.zeros([self.net_dim[0], self.net_dim[1]])\n",
    "        self.init_radius = np.max(self.net_dim)/2\n",
    "        self.umatrix = np.zeros([self.net_dim[0], self.net_dim[1]])\n",
    "        self.umat_2 = np.zeros([self.net_dim[0]*2 - 1, self.net_dim[1]*2 - 1])\n",
    "        self.bmu_list = None\n",
    "        self.label_map = np.zeros([self.net_dim[0], self.net_dim[1]]) + -1\n",
    "        self.classif_log=None # matrix for storing data for each neu, #times it becomed BMU for each class\n",
    "        self.neu_class=np.zeros([m*n])# store class for each neuron \n",
    "        self.trainAccuracy=None\n",
    "        self.testAccuracy=None\n",
    "\n",
    "    def fit_model(self, train_x, train_y=None, init_lr=0.1, epochs=100):\n",
    "        # Initializing the weight matrix\n",
    "        self.weights = np.random.random((self.net_dim[0], self.net_dim[1], train_x.shape[1]))\n",
    "        self.bmu_list = np.zeros(train_x.shape[0])\n",
    "        time_constant = epochs/np.log(self.init_radius)\n",
    "        \n",
    "        if not (train_y is None):\n",
    "            #print(\"np.unique(train_y).shape[0]\",np.unique(train_y).shape[0])\n",
    "            self.classif_log=np.zeros([np.unique(train_y).shape[0] , self.net_dim[0]*self.net_dim[1]])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('Processing epoch:{}'.format(epoch))\n",
    "            \"\"\"decay the radius and the learning rate\"\"\"\n",
    "            if epoch != 0:\n",
    "                radius = SOM.decay_radius(self.init_radius,epoch, time_constant)\n",
    "                lr = SOM.decay_learning_rate(init_lr, epoch, epochs)\n",
    "\n",
    "            else:\n",
    "                radius = self.init_radius\n",
    "                lr = init_lr\n",
    "\n",
    "            #print(\"train_x.shape[0]\",train_x.shape[0])            \n",
    "            randlist=random.sample(range(train_x.shape[0]), train_x.shape[0])\n",
    "            for t in range(train_x.shape[0]):\n",
    "\n",
    "                # Picking a random pattern\n",
    "                r = randlist[t]                \n",
    "                # Finding the bmu for the random pattern\n",
    "                bmu_index, bmu = self.find_bmu(train_x[r, :])\n",
    "                #print(\"bmu_index : \",bmu_index) \n",
    "                self.bmu_list[r] = bmu_index[0]*self.weights.shape[1] + bmu_index[1]\n",
    "                #print(\"self.bmu_list[r] :: \",self.bmu_list[r])\n",
    "\n",
    "                # updating the weights\n",
    "                for i in range(self.net_dim[0]):\n",
    "                    for j in range(self.net_dim[1]):\n",
    "                        w = self.weights[i, j, :]\n",
    "                        w_bmu_dist = SOM.get_distance(w, bmu)\n",
    "\n",
    "                        h = self.get_gaussian_membership(w_bmu_dist, radius)\n",
    "                        delta_w = (lr * h * (train_x[r, :] - w))\n",
    "                        new_w = w + delta_w\n",
    "                        self.weights[i, j, :] = new_w                        \n",
    "                        if np.isnan(self.weights).any():\n",
    "                            print(' weights NaN at epoch {}'.format(epoch))\n",
    "\n",
    "                            return\n",
    "                        # print('\\n')\n",
    "            ##\n",
    "            if not (train_y is None) :       \n",
    "                for t in range(train_x.shape[0]):\n",
    "                    bmu_index, bmu = self.find_bmu(train_x[t, :])\n",
    "                    self.bmu_list[t] = bmu_index[0]*self.weights.shape[1] + bmu_index[1]\n",
    "                    #print(\"self.bmu_list[r]\",self.bmu_list[r])\n",
    "                    self.classif_log[int(train_y[t]),int(self.bmu_list[t])]=(self.classif_log[int(train_y[t]),int(self.bmu_list[t])])+1\n",
    "                print(\"Train Accuracy\",(sum(self.classif_log.max(axis=0))*100)/ (train_x.shape[0]) )\n",
    "                self.classif_log=np.zeros([np.unique(train_y).shape[0] , self.net_dim[0]*self.net_dim[1]])\n",
    "                self.trainAccuracy=(sum(self.classif_log.max(axis=0))*100)/ (train_x.shape[0])\n",
    " \n",
    "        if not (train_y is None) :       \n",
    "            for t in range(train_x.shape[0]):\n",
    "                bmu_index, bmu = self.find_bmu(train_x[t, :])\n",
    "                self.bmu_list[t] = bmu_index[0]*self.weights.shape[1] + bmu_index[1]\n",
    "                #print(\"self.bmu_list[r]\",self.bmu_list[r])\n",
    "                \"\"\"                    \n",
    "                print(\"bla\", bmu_index,bmu ,\" t\",t) # bla [0, 2] [-0.002496    0.00031421 -0.00322767] print(\"Train y val :\",int(train_y[t]))\n",
    "                print(\"self.bmu_list :\",int(self.bmu_list[t]))\n",
    "                \"\"\"\n",
    "                #print(\"int(train_y[t])\",int(train_y[t]) )\n",
    "                #print(\"int(self.bmu_list[t])\",int(self.bmu_list[t]))\n",
    "                self.classif_log[int(train_y[t]),int(self.bmu_list[t])]=(self.classif_log[int(train_y[t]),int(self.bmu_list[t])])+1\n",
    "                        \n",
    "            print(\"Check \\n\",self.classif_log)        \n",
    "            self.neu_class=self.classif_log.argmax(axis=0) \n",
    "            print(\"self.neu_class\",self.neu_class)\n",
    "            print(\"Train Accuracy\",(sum(self.classif_log.max(axis=0))*100)/ (train_x.shape[0]) )\n",
    "            self.trainAccuracy=(sum(self.classif_log.max(axis=0))*100)/ (train_x.shape[0])\n",
    "        self.set_umatrix()\n",
    "        self.set_hitmap()\n",
    "        #now test for accuracy\n",
    "        #if train_y is not None:\n",
    "            #self.set_label_map(train_y)#this needs checking cz it prints something \n",
    "        \n",
    "    def test_model(self, test_x, test_y=None):\n",
    "        \n",
    "        correct=0;\n",
    "        \n",
    "        for t in range(test_x.shape[0]):\n",
    "            bmu_index, bmu = self.find_bmu(test_x[t, :])\n",
    "            neu_no=bmu_index[0]*self.weights.shape[1] + bmu_index[1]\n",
    "            #print(\"neu_no : \",neu_no)\n",
    "            class_label=self.neu_class[neu_no]\n",
    "\n",
    "            if(class_label==test_y[t]):\n",
    "                correct=correct+1\n",
    "        print(\"Test Accuracy : \", (correct*100/test_x.shape[0]))\n",
    "        self.testAccuracy=(correct*100/test_x.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def find_bmu(self, x):\n",
    "        min_dist = float('inf')\n",
    "        bmu_index = [-1, -1]\n",
    "        bmu = self.weights[0, 0, :]\n",
    "        for i in range(self.net_dim[0]):\n",
    "            for j in range(self.net_dim[1]):\n",
    "                w = self.weights[i, j, :]\n",
    "                d = SOM.get_distance(x, w)\n",
    "                #print(\"d : \",d)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    bmu_index = [i, j]\n",
    "                    bmu = w\n",
    "\n",
    "        return bmu_index, bmu\n",
    "    \n",
    "    def find_bmu_list(self, x):\n",
    "        min_dist = float('inf')\n",
    "        bmu_index = [-1, -1]\n",
    "        bmu = self.weights[0, 0, :]\n",
    "        dis_list=[]\n",
    "        for i in range(self.net_dim[0]):\n",
    "            for j in range(self.net_dim[1]):\n",
    "                w = self.weights[i, j, :]\n",
    "                d = SOM.get_distance(x, w)\n",
    "                dis_list.append(d)\n",
    "                #print(\"d : \",d)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    bmu_index = [i, j]\n",
    "                    bmu = w\n",
    "        s = dis_list\n",
    "        sorted_list=sorted(range(len(s)), key=lambda k: s[k])\n",
    "        #print(dis_list)\n",
    "        #print(bmu_index)\n",
    "        #print(dis_list[sorted_list[0]])\n",
    "        #print(min_dist)\n",
    "        return bmu_index, bmu, sorted_list\n",
    "\n",
    "    def get_gaussian_membership(self, squared_distance_from_bmu, radius):\n",
    "        h = np.exp(-squared_distance_from_bmu/(2*(radius**2)))\n",
    "        # h = np.exp(squared_distance_from_bmu/(2*(radius**2)))\n",
    "        # print('h:{}'.format(h))\n",
    "        return h\n",
    "\n",
    "    def set_hitmap(self):\n",
    "        unique, counts = np.unique(self.bmu_list, return_counts=True)\n",
    "        #print(\"unique\",unique)\n",
    "        #print(\"counts\",counts)\n",
    "        bmu_dict = dict(zip(unique.astype(int), counts))\n",
    "\n",
    "        for bmu, count in bmu_dict.items():\n",
    "            i = int(bmu / self.label_map.shape[1])\n",
    "            j = bmu % self.label_map.shape[1]\n",
    "            self.hit_map[i, j] = count\n",
    "            \n",
    "        #print(np.shape(self.hit_map))\n",
    "\n",
    "    def set_label_map(self, train_y):\n",
    "        # print(self.label_map)\n",
    "\n",
    "        unique, counts = np.unique(self.bmu_list, return_counts=True)\n",
    "        bmu_dict = dict(zip(unique.astype(int), counts))\n",
    "        print(bmu_dict)\n",
    "\n",
    "        for bmu, count in bmu_dict.items():\n",
    "            # print('bmu: {}, count: {}'.format(bmu, count))\n",
    "            # print(self.bmu_list[self.bmu_list == bmu])\n",
    "            idx = np.where(self.bmu_list == bmu)\n",
    "            unique_labels, label_counts = np.unique(train_y[idx], return_counts=True)\n",
    "\n",
    "            labels_dict = dict(zip(unique_labels.astype(int), label_counts))\n",
    "            # print(labels_dict)\n",
    "\n",
    "            import operator\n",
    "            # print(max(labels_dict.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "            i = int(bmu / self.label_map.shape[1])\n",
    "            j = bmu % self.label_map.shape[1]\n",
    "\n",
    "            self.label_map[i, j] = max(labels_dict.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "    @classmethod\n",
    "    def get_distance(cls, x1, x2):\n",
    "        return np.sum((x1 - x2) ** 2)\n",
    "\n",
    "    @classmethod\n",
    "    def decay_radius(cls, init_radius, e, time_constant):\n",
    "        return init_radius * np.exp(-e / time_constant)\n",
    "\n",
    "    @classmethod\n",
    "    def decay_learning_rate(cls, init_lr, e, num_epochs):\n",
    "        return init_lr * np.exp(-e / num_epochs)\n",
    "\n",
    "    @classmethod\n",
    "    def draw_square(cls, x, y, dim, color):\n",
    "        rect = plt.Rectangle((x, y), dim, dim, fc=color)\n",
    "\n",
    "        return rect\n",
    "\n",
    "    def show_hitmap(self, show_hits=False):\n",
    "        print(\"show hit map\")\n",
    "        #fig, ax = plt.subplots()\n",
    "        plt.figure()\n",
    "        plt.imshow(self.hit_map, cmap='jet')\n",
    "        plt.title('Hit map')\n",
    "        plt.colorbar()        \n",
    "        if show_hits:\n",
    "            for i in range(self.net_dim[0]):\n",
    "                for j in range(self.net_dim[1]):\n",
    "                    c = self.hit_map[j,i]\n",
    "                    plt.text(i, j, str(c), va='center', ha='center')    \n",
    "        plt.show()\n",
    "        \n",
    "    def show_umatrix(self):\n",
    "        plt.figure()\n",
    "        plt.imshow(self.umatrix, cmap='jet')\n",
    "        plt.title('U-Matrix')\n",
    "        plt.colorbar()\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "\n",
    "    def set_umatrix(self, nbhd='neumann'):\n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                self.umatrix[i, j] = self.calculate_average_distance([i, j], nbhd=nbhd)\n",
    "\n",
    "    def calculate_average_distance(self, index, nbhd='neumann'):\n",
    "        avg_distance = -1\n",
    "\n",
    "        if nbhd == 'neumann':\n",
    "            n = np.zeros([4])\n",
    "            div = 4\n",
    "            x = index[0]\n",
    "            y = index[1]\n",
    "            # Above\n",
    "            if (x - 1) >= 0:\n",
    "                n[0] = np.sqrt(SOM.get_distance(self.weights[x - 1, y, :], self.weights[x, y, :]))\n",
    "            else:\n",
    "                div -= 1\n",
    "            # Below\n",
    "            if (x + 1) < self.weights.shape[0]:\n",
    "                n[1] = np.sqrt(SOM.get_distance(self.weights[x + 1, y, :], self.weights[x, y, :]))\n",
    "            else:\n",
    "                div -= 1\n",
    "            # Left\n",
    "            if (y - 1) >= 0:\n",
    "                n[2] = np.sqrt(SOM.get_distance(self.weights[x, y - 1, :], self.weights[x, y, :]))\n",
    "            else:\n",
    "                div -= 1\n",
    "            # Right\n",
    "            if (y + 1) < self.weights.shape[1]:\n",
    "                n[3] = np.sqrt(SOM.get_distance(self.weights[x, y + 1, :], self.weights[x, y, :]))\n",
    "            else:\n",
    "                div -= 1\n",
    "\n",
    "            avg_distance = np.sum(n) / div\n",
    "        elif nbhd == 'moore':\n",
    "            # TODO implement for moore neighborhood\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            print('Not a valid Neighborhood')\n",
    "\n",
    "        return avg_distance\n",
    "\n",
    "    def set_umat_2(self):\n",
    "        # TODO: Need to optimize this code\n",
    "        r = 0\n",
    "        for i in range(self.net_dim[0]):\n",
    "            c = 0\n",
    "            for j in range(self.net_dim[1]):\n",
    "                if c+1 < self.umat_2.shape[1]:\n",
    "                    self.umat_2[r, c+1] = SOM.get_distance(self.weights[i, j, :], self.weights[i, j+1, :])\n",
    "\n",
    "                if r+1 < self.umat_2.shape[0]:\n",
    "                    self.umat_2[r+1, c] = SOM.get_distance(self.weights[i, j, :], self.weights[i+1, j, :])\n",
    "                c = c+2\n",
    "            r = r+2\n",
    "\n",
    "        temp = np.zeros([self.umat_2.shape[0], self.umat_2.shape[1], self.weights.shape[2]])\n",
    "        r = 0\n",
    "        for i in range(0, self.umat_2.shape[0], 2):\n",
    "            c = 0\n",
    "            for j in range(0, self.umat_2.shape[1],2):\n",
    "                temp[i, j] = self.weights[r, c, :]\n",
    "\n",
    "                c += 1\n",
    "            r += 1\n",
    "\n",
    "        for i in range(self.umat_2.shape[0]):\n",
    "            for j in range(self.umat_2.shape[1]):\n",
    "                if i % 2 != 0 and j % 2 != 0:\n",
    "                    self.umat_2[i, j] = (SOM.get_distance(temp[i-1, j-1, :], temp[i+1, j+1, :]) +\n",
    "                                         SOM.get_distance(temp[i-1, j+1, :], temp[i+1, j-1, :]))/2\n",
    "\n",
    "    def show_umatrx_2(self, size=2):\n",
    "        max_dist = np.max(self.umat_2)\n",
    "        print(max_dist)\n",
    "        cmap = matplotlib.pyplot.get_cmap('jet', lut=256)\n",
    "        plt.figure()\n",
    "        plt.imshow(self.umat_2, cmap='jet')\n",
    "        plt.title('U-Matrix')\n",
    "        plt.colorbar()\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "        \n",
    "    def print_save_weights(self,fileName):\n",
    "        \"\"\"\n",
    "        for i in range(self.weights.shape[0]):\n",
    "            print(self.weights[i,:]) \n",
    "        \"\"\"  \n",
    "        np.save(fileName,self.weights)\n",
    "        #print(\"load: \\n\", np.load('maximums.npy'))\n",
    "\n",
    "def standardize_data(X):\n",
    "    x_norm = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return x_norm\n",
    "    from tempfile import TemporaryFile   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 44)\n"
     ]
    }
   ],
   "source": [
    "dataset_name='KDDTrainset.csv'\n",
    "data1=genfromtxt(dataset_name,delimiter=\",\")\n",
    "print(np.shape(data1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67343,)\n",
      "(67343, 41)\n",
      "(58630,)\n",
      "(58630, 44)\n",
      "(125973, 44)\n"
     ]
    }
   ],
   "source": [
    "ii = np.where(data1[:,43] == 1)[0]\n",
    "normal_data=(data1[ii, 0:41])\n",
    "\n",
    "print(np.shape(ii))\n",
    "print(np.shape(normal_data))\n",
    "\n",
    "ii = np.where(data1[:,43] != 1)[0]\n",
    "attack_data=(data1[ii, :])\n",
    "\n",
    "print(np.shape(ii))\n",
    "print(np.shape(attack_data))\n",
    "\n",
    "print(np.shape(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 41)\n",
      "(10000, 41)\n",
      "(20000, 41)\n"
     ]
    }
   ],
   "source": [
    "train_x=normal_data[0:50000,0:41]\n",
    "print(np.shape(train_x))\n",
    "normal_test=normal_data[50001:60001,0:41]\n",
    "print(np.shape(normal_test))\n",
    "attack_test=attack_data[0:20000,0:41]\n",
    "print(np.shape(attack_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch:0\n",
      "Processing epoch:1\n",
      "Processing epoch:2\n",
      "Processing epoch:3\n",
      "Processing epoch:4\n"
     ]
    }
   ],
   "source": [
    "som = SOM(8, 8)\n",
    "som.fit_model(train_x, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 41)\n"
     ]
    }
   ],
   "source": [
    "filename_for_weight='weight1'\n",
    "som.print_save_weights(filename_for_weight)\n",
    "points=np.load('bla.npy')\n",
    "print(np.shape(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 41)\n"
     ]
    }
   ],
   "source": [
    "BMUList=[]\n",
    "for i in range(0,8):\n",
    "    for j in range(0,8):        \n",
    "        BMUList.append(points[i,j,:])\n",
    "        \n",
    "print(np.shape(BMUList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "[0 0 4 0 0 0 0 5 0 0 7 0 6 0 0 0 3 0 8 9 0 0 0 9 0 0 0 0 1 2 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "no_clusters=10\n",
    "kmeans = KMeans(n_clusters=no_clusters, random_state=0).fit(BMUList)\n",
    "labels_of_BMUs=kmeans.labels_\n",
    "print(np.shape(labels_of_BMUs))\n",
    "print(labels_of_BMUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1. ...,  1.  0.  0.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#for each data point store the cluster label\n",
    "data_cluster_list=np.zeros(50000);\n",
    "for i in range(0,50000):\n",
    "    bmu_index, bmu = som.find_bmu(train_x[i, :])    \n",
    "    data_cluster_list[i]=(labels_of_BMUs[((bmu_index[0])*8) +bmu_index[1]]);\n",
    "print(data_cluster_list) \n",
    "print(type(data_cluster_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________\n",
      "[    0     1     3 ..., 49996 49998 49999]\n",
      "________________________\n",
      "[    2    51    91   137   199   205   233   273   276   367   398   401\n",
      "   402   412   442   458   495   567   570   575   615   670   712   714\n",
      "   735   769   807   819   833   878   913   962  1121  1165  1182  1268\n",
      "  1291  1326  1429  1440  1474  1539  1549  1591  1602  1715  1788  1829\n",
      "  1852  2037  2185  2321  2449  2576  2708  2740  2832  2904  2925  2935\n",
      "  2956  2994  3136  3171  3188  3196  3205  3224  3247  3260  3355  3359\n",
      "  3383  3445  3470  3502  3576  3579  3673  3707  3784  3803  3941  3952\n",
      "  4080  4119  4147  4182  4272  4283  4285  4297  4314  4316  4334  4383\n",
      "  4390  4402  4415  4485  4517  4567  4604  4630  4707  4876  4895  4982\n",
      "  5121  5149  5245  5254  5282  5303  5304  5316  5442  5468  5522  5536\n",
      "  5553  5661  5810  5859  5885  5910  6036  6107  6154  6177  6188  6228\n",
      "  6385  6495  6506  6536  6610  6630  6665  6690  6770  6855  6882  7087\n",
      "  7112  7239  7269  7299  7337  7412  7449  7481  7504  7539  7689  7696\n",
      "  7707  7896  7909  7921  8035  8044  8096  8114  8116  8229  8232  8260\n",
      "  8275  8276  8320  8337  8346  8492  8507  8519  8548  8582  8632  8639\n",
      "  8778  8915  8920  9075  9326  9401  9414  9461  9499  9616  9729  9823\n",
      "  9864  9888  9898  9953 10008 10197 10212 10410 10471 10566 10568 10575\n",
      " 10801 10807 10812 10898 11009 11025 11051 11096 11106 11130 11133 11165\n",
      " 11309 11337 11404 11504 11538 11542 11635 11642 11649 11736 11862 11887\n",
      " 11967 11985 12108 12125 12129 12223 12244 12261 12275 12379 12567 12628\n",
      " 12772 12861 12983 13042 13044 13049 13076 13168 13180 13245 13284 13353\n",
      " 13425 13456 13600 13630 13771 13925 13959 14006 14011 14041 14139 14176\n",
      " 14240 14247 14249 14286 14299 14309 14322 14417 14494 14502 14520 14585\n",
      " 14640 14650 14666 14781 14829 14831 14897 14930 14945 14959 15000 15015\n",
      " 15034 15044 15152 15305 15311 15361 15398 15444 15501 15654 15685 15698\n",
      " 15740 15813 15879 15994 16012 16034 16122 16128 16208 16235 16283 16423\n",
      " 16484 16526 16794 16931 16949 17043 17060 17107 17169 17272 17311 17351\n",
      " 17606 17610 17684 17735 17802 17815 17818 17820 17830 17857 17861 17912\n",
      " 17950 18028 18034 18049 18069 18101 18182 18320 18366 18488 18499 18624\n",
      " 18645 18728 18750 18896 19038 19050 19073 19098 19116 19164 19191 19573\n",
      " 19679 19789 19818 19842 19847 19993 20033 20096 20111 20199 20248 20278\n",
      " 20298 20362 20401 20699 20708 20715 20774 20919 20931 20955 20985 20996\n",
      " 21043 21085 21140 21188 21216 21346 21363 21414 21487 21488 21508 21653\n",
      " 21659 21666 21670 21719 21734 21739 21754 21831 21833 21881 21940 22181\n",
      " 22229 22272 22282 22307 22461 22479 22492 22508 22534 22544 22617 22767\n",
      " 22775 22780 22787 22961 22974 23019 23054 23070 23203 23272 23384 23477\n",
      " 23500 23534 23552 23688 23709 23718 23762 23763 23812 23922 23981 23998\n",
      " 24012 24234 24262 24286 24307 24384 24413 24474 24510 24513 24660 24676\n",
      " 24852 24902 24978 25000 25061 25151 25163 25190 25213 25251 25280 25281\n",
      " 25286 25352 25390 25468 25502 25580 25596 25616 25721 25807 25860 25907\n",
      " 25908 25924 25959 26017 26032 26084 26123 26146 26156 26167 26218 26221\n",
      " 26235 26336 26354 26573 26620 26821 26837 26883 26945 27043 27060 27109\n",
      " 27111 27126 27150 27196 27211 27213 27224 27304 27355 27372 27378 27384\n",
      " 27424 27431 27438 27447 27527 27576 27591 27596 27598 27645 27720 27723\n",
      " 27802 27810 27919 27951 28000 28047 28126 28179 28181 28196 28250 28254\n",
      " 28341 28461 28469 28570 28588 28679 28692 28743 28760 28838 28888 28943\n",
      " 28963 29102 29115 29160 29168 29259 29261 29307 29477 29581 29631 29632\n",
      " 29645 29654 29694 29795 29805 29823 30040 30065 30072 30082 30131 30133\n",
      " 30141 30186 30218 30227 30228 30244 30260 30277 30323 30326 30361 30379\n",
      " 30390 30513 30642 30717 30752 31018 31020 31057 31312 31316 31375 31458\n",
      " 31483 31529 31638 31677 31697 31767 31841 31909 32038 32049 32140 32162\n",
      " 32163 32220 32248 32249 32256 32303 32307 32324 32332 32410 32481 32511\n",
      " 32606 32687 32777 32825 32852 32856 32943 33000 33016 33043 33157 33172\n",
      " 33200 33239 33249 33267 33302 33428 33456 33464 33639 33643 33675 33771\n",
      " 33842 33913 34034 34058 34081 34141 34236 34237 34294 34413 34528 34533\n",
      " 34581 34616 34624 34640 34672 34755 34767 34893 35012 35060 35247 35349\n",
      " 35366 35392 35457 35460 35580 35602 35605 35692 35699 35732 35759 35772\n",
      " 35782 35850 35907 35921 35944 36039 36058 36078 36104 36256 36394 36527\n",
      " 36592 36604 36892 36905 36909 37016 37028 37037 37054 37133 37200 37227\n",
      " 37228 37242 37283 37375 37633 37727 37739 37754 37781 37794 37798 37835\n",
      " 37956 38030 38180 38221 38273 38281 38290 38401 38451 38510 38548 38556\n",
      " 38574 38588 38686 38860 39034 39060 39090 39234 39494 39524 39545 39591\n",
      " 39688 39749 39778 39784 39866 39890 39944 40093 40120 40181 40275 40331\n",
      " 40355 40412 40504 40512 40606 40840 40859 40862 40968 40999 41021 41072\n",
      " 41087 41092 41132 41280 41306 41448 41455 41476 41544 41592 41595 41716\n",
      " 41739 41764 41843 41871 41880 41891 41916 41930 41990 42018 42026 42038\n",
      " 42086 42131 42165 42193 42213 42248 42353 42405 42414 42454 42462 42515\n",
      " 42640 42668 42720 42799 42812 42931 43033 43080 43097 43105 43119 43120\n",
      " 43193 43377 43457 43478 43517 43577 43603 43604 43754 43861 43865 43941\n",
      " 44026 44059 44160 44178 44192 44215 44227 44316 44332 44387 44418 44484\n",
      " 44488 44495 44646 44664 44688 44724 44816 44820 44821 44949 44980 45004\n",
      " 45054 45273 45390 45404 45516 45576 45589 45602 45621 45650 45686 45688\n",
      " 45805 45851 45965 46036 46054 46110 46175 46188 46265 46271 46341 46397\n",
      " 46398 46405 46592 46610 46621 46715 46778 46845 46917 46942 46946 46977\n",
      " 47215 47313 47329 47330 47345 47379 47399 47504 47635 47708 47734 47779\n",
      " 47820 47876 47924 47948 47969 48014 48016 48031 48225 48260 48312 48354\n",
      " 48366 48375 48418 48477 48691 48766 48768 48813 48826 48866 48875 48880\n",
      " 48926 48929 48957 48982 49163 49176 49193 49292 49300 49332 49342 49377\n",
      " 49463 49522 49528 49542 49602 49610 49667 49748 49882 49894 49936 49937\n",
      " 49997]\n",
      "________________________\n",
      "[  164   549   627   725   753   846   849   871   889   933  1499  1624\n",
      "  1946  2105  2108  2217  2226  2252  2405  2565  2590  2640  2803  3008\n",
      "  3108  3344  3390  3804  4079  4096  4239  4906  5007  5280  5386  5396\n",
      "  6246  6356  6875  7339  7592  7643  7971  8110  8278  8535  8636  9051\n",
      "  9485  9562  9701  9725  9839  9938 10132 10207 10251 10275 10664 10750\n",
      " 10877 11250 11399 11443 11871 12259 12521 12711 12766 12799 13113 13339\n",
      " 13606 13867 14110 14190 15071 15315 16016 16030 16075 16297 16498 16504\n",
      " 16557 16742 17816 18192 18243 18960 19036 19207 19499 20286 20287 20358\n",
      " 20580 21013 21014 21354 21521 22311 22314 22511 22520 22644 22694 22941\n",
      " 22981 23004 23423 23602 24122 24303 24609 25004 25358 25415 25793 25997\n",
      " 26725 27129 27154 27199 27348 27764 28183 28208 28402 28406 28532 28748\n",
      " 29137 29219 29339 29430 29652 29897 29978 30296 30507 30664 30888 31772\n",
      " 32052 32070 32087 32092 32359 32642 32646 32758 33127 33130 33156 33178\n",
      " 33227 33501 33637 33837 33922 34280 34442 34549 34653 34982 35227 35683\n",
      " 35821 35883 35957 36030 36359 36499 36765 36856 36970 37497 37809 37820\n",
      " 38009 38122 38456 38475 38490 38618 38675 39005 39599 39705 39925 40042\n",
      " 40049 40053 40070 40089 40121 40282 40370 40871 41112 41279 41369 41597\n",
      " 42155 42224 42548 42632 42928 43012 43269 43524 43556 44036 44100 44371\n",
      " 44423 44490 44877 44994 45053 45318 45432 45655 45713 45863 46515 46590\n",
      " 47224 47390 47737 47864 47957 48117 48274 48628 48751 48786 48894 49187\n",
      " 49200 49995]\n",
      "________________________\n",
      "[    5    10   115   121   259   264   265   371   403   455   460   501\n",
      "   531   541   556   655   687   705   716   826   946  1003  1025  1066\n",
      "  1114  1161  1190  1248  1256  1450  1497  1613  1621  1684  1835  1838\n",
      "  1906  1974  1998  2032  2174  2188  2193  2212  2220  2419  2478  2535\n",
      "  2566  2636  2722  2768  2898  2944  3119  3231  3232  3326  3436  3449\n",
      "  3570  3574  3769  3825  4011  4030  4060  4169  4183  4217  4380  4480\n",
      "  4513  4648  4769  4901  4911  4935  4946  4947  5110  5119  5170  5294\n",
      "  5392  5655  5721  5776  5785  5915  5922  5976  5989  6072  6094  6101\n",
      "  6165  6275  6468  6473  6483  6675  6771  6803  6853  6870  6881  6932\n",
      "  6972  7021  7085  7179  7220  7372  7433  7533  7812  7866  7888  8003\n",
      "  8043  8139  8213  8472  8585  8605  8685  8765  8802  8914  8962  8984\n",
      "  9151  9176  9232  9280  9450  9502  9565  9576  9617  9640  9655  9766\n",
      "  9813  9862  9981 10040 10057 10101 10210 10228 10266 10269 10295 10353\n",
      " 10451 10579 10584 10610 10672 10678 10744 10795 10866 10917 10975 11065\n",
      " 11358 11410 11449 11556 11802 11826 11892 11926 11961 12005 12147 12158\n",
      " 12161 12183 12210 12276 12332 12631 12655 12661 12685 12855 13037 13067\n",
      " 13087 13129 13157 13185 13205 13409 13448 13720 13723 13813 13911 14000\n",
      " 14062 14399 14436 14467 14491 14496 14522 14564 14565 14709 14774 14871\n",
      " 14951 15028 15125 15210 15214 15348 15382 15383 15407 15567 15639 15700\n",
      " 15851 15887 15913 16055 16072 16139 16249 16270 16291 16303 16329 16412\n",
      " 16502 16580 16647 16654 16679 16684 16872 16919 16954 17019 17188 17411\n",
      " 17500 17547 17621 17918 17975 18000 18080 18083 18299 18401 18419 18428\n",
      " 18443 18472 18474 18604 18642 18671 18942 18962 19056 19212 19219 19362\n",
      " 19395 19415 19445 19462 19494 19645 19704 19819 19917 19934 19947 19955\n",
      " 19998 20145 20275 20312 20361 20412 20444 20557 20588 20654 20761 20807\n",
      " 20842 20902 21047 21144 21179 21215 21218 21261 21262 21271 21274 21320\n",
      " 21331 21588 21685 21752 21875 21915 21974 22047 22424 22593 22627 22667\n",
      " 22722 22878 22886 22889 23165 23295 23326 23468 23482 23488 23586 23739\n",
      " 23785 23851 23943 23983 24085 24118 24120 24261 24269 24357 24378 24478\n",
      " 24482 24491 24543 24626 24647 24701 24721 24775 24781 24830 24877 24911\n",
      " 24952 25018 25025 25117 25215 25324 25396 25431 25498 25513 25514 25522\n",
      " 25600 25654 25911 25923 25973 26067 26071 26085 26094 26131 26183 26484\n",
      " 26556 26751 26827 26922 26936 27059 27086 27197 27286 27370 27396 27446\n",
      " 27486 27497 27578 27687 27739 27848 27853 27953 28008 28037 28088 28092\n",
      " 28290 28310 28322 28350 28373 28424 28426 28468 28470 28569 28575 28685\n",
      " 28703 28786 28941 29206 29210 29237 29350 29665 29681 29769 29842 29858\n",
      " 29890 29898 29917 29970 29990 30075 30267 30303 30535 30546 30615 30769\n",
      " 30793 30883 31246 31332 31355 31369 31399 31406 31424 31508 31577 31630\n",
      " 31774 31789 31936 31944 32024 32174 32355 32399 32416 32484 32513 32539\n",
      " 32552 32651 32845 32921 32939 32950 33035 33134 33171 33223 33280 33292\n",
      " 33434 33550 33668 33845 33902 33912 34050 34144 34150 34302 34336 34385\n",
      " 34544 34579 34580 34689 34771 34815 34866 34902 34986 35000 35146 35177\n",
      " 35191 35316 35396 35430 35535 35574 35838 35854 35912 36210 36276 36388\n",
      " 36407 36663 36829 36927 36978 37053 37147 37173 37189 37311 37339 37360\n",
      " 37418 37529 37659 37684 37695 37773 37945 38033 38039 38059 38099 38146\n",
      " 38159 38166 38179 38292 38364 38403 38471 38496 38657 38688 38785 38809\n",
      " 38865 38927 39036 39057 39070 39140 39285 39380 39471 39576 39810 39815\n",
      " 39953 40220 40290 40489 40563 40567 40605 40647 40658 40698 40705 40716\n",
      " 40723 40845 40866 40986 41048 41191 41324 41452 41532 41672 41779 41862\n",
      " 41893 42073 42176 42378 42468 42524 42532 42574 42750 42752 42762 42870\n",
      " 43022 43041 43157 43229 43362 43440 43484 43535 43545 43602 43607 43616\n",
      " 43638 43655 43683 43689 43712 43724 43751 43785 43801 43827 43939 43947\n",
      " 44123 44154 44273 44414 44448 44464 44494 44674 44884 45036 45072 45091\n",
      " 45102 45229 45275 45278 45466 45484 45550 45642 45671 45730 45865 45911\n",
      " 46017 46160 46238 46353 46430 46504 46749 46999 47077 47245 47255 47268\n",
      " 47319 47469 47518 47569 47608 47610 47618 47647 47677 47744 47868 47905\n",
      " 47908 48019 48070 48149 48245 48363 48462 48682 48695 48698 48706 48761\n",
      " 49053 49118 49142 49205 49218 49257 49334 49347 49380 49419 49474 49481\n",
      " 49518 49659 49736 49746 49750 49769 49798 49810 49899 49990 49991]\n",
      "________________________\n",
      "[  551   673  1056  2521  3156  3505  3991  5050  6705  8702  9634 10404\n",
      " 10938 11102 12110 12563 12786 13270 14033 15723 15941 17450 17455 19012\n",
      " 20163 21585 22430 23084 23976 25099 27947 29669 31150 31591 31745 31758\n",
      " 32297 33234 33444 33513 34319 35475 35593 40323 40435 41380 41482 42642\n",
      " 43211 43423 44960 46641 47095 48066 48256 48268 48869 49473]\n",
      "________________________\n",
      "[   54    75    83   112   133   147   179   293   349   456   571   587\n",
      "   607   832   838  1082  1101  1229  1258  1279  1304  1379  1398  1455\n",
      "  1467  1616  1740  1751  1795  1797  1826  1828  2069  2089  2146  2200\n",
      "  2278  2309  2367  2378  2399  2413  2469  2508  2536  2561  2581  2650\n",
      "  2713  2730  2735  2742  2812  2824  2896  2899  2961  3005  3105  3123\n",
      "  3129  3255  3264  3315  3320  3347  3459  3577  3580  3593  3650  3656\n",
      "  3702  3766  3792  3820  3830  3836  3870  3887  3908  3961  4115  4216\n",
      "  4345  4427  4538  4653  4706  4726  4754  4809  4853  4889  4908  4943\n",
      "  5035  5126  5201  5208  5248  5251  5256  5258  5354  5374  5379  5525\n",
      "  5589  5711  5820  5862  5890  6035  6112  6132  6189  6220  6371  6403\n",
      "  6424  6575  6795  6831  6838  6976  7022  7050  7095  7097  7133  7283\n",
      "  7395  7515  7536  7538  7566  7612  7630  7702  7820  7987  8004  8042\n",
      "  8128  8219  8265  8282  8316  8378  8485  8568  8602  8612  8641  8741\n",
      "  8754  8760  8773  8786  8888  8985  8995  9023  9082  9114  9117  9135\n",
      "  9267  9311  9359  9515  9572  9691  9786  9943  9948 10005 10007 10055\n",
      " 10078 10138 10193 10232 10332 10383 10390 10547 10553 10602 10621 10645\n",
      " 10680 10704 10814 10914 10915 11026 11042 11216 11236 11292 11293 11459\n",
      " 11510 11643 11701 11791 11807 11825 11844 11895 11933 11964 12007 12089\n",
      " 12135 12319 12356 12371 12403 12406 12489 12544 12587 12727 12866 12956\n",
      " 13121 13193 13259 13309 13402 13428 13529 13580 13664 13732 13743 13747\n",
      " 13797 13989 14042 14047 14057 14114 14199 14228 14482 14549 14674 14792\n",
      " 14819 14846 14870 14881 14967 15004 15065 15070 15072 15092 15116 15133\n",
      " 15185 15191 15283 15302 15306 15495 15583 15625 15664 15669 15690 15775\n",
      " 15816 15832 15838 15857 15966 15970 15998 16041 16043 16214 16290 16413\n",
      " 16445 16469 16500 16634 16639 16689 16851 16885 16976 16997 17038 17046\n",
      " 17098 17129 17196 17224 17256 17306 17367 17424 17590 17620 18030 18110\n",
      " 18127 18195 18235 18283 18330 18343 18383 18465 18661 18734 18776 18791\n",
      " 18800 18805 18829 18919 19019 19076 19129 19137 19167 19236 19279 19280\n",
      " 19452 19487 19570 19575 19666 19803 19811 19827 19906 19936 19965 19984\n",
      " 19999 20037 20210 20220 20320 20324 20360 20367 20402 20420 20437 20689\n",
      " 20844 20879 20914 20932 20946 20950 21026 21111 21172 21174 21223 21370\n",
      " 21502 21529 21599 21636 21810 21880 21929 22432 22437 22507 22622 22638\n",
      " 22737 22748 22765 22768 22862 22919 22980 23172 23189 23253 23259 23268\n",
      " 23333 23372 23526 23557 23567 23735 23895 23956 23982 24051 24070 24105\n",
      " 24276 24297 24375 24400 24550 24572 24743 24757 24758 24803 24848 24981\n",
      " 24985 25027 25043 25046 25048 25114 25185 25236 25258 25261 25271 25272\n",
      " 25426 25593 25650 25672 25824 26081 26107 26220 26224 26364 26462 26468\n",
      " 26521 26592 26688 26758 26760 26791 26829 26889 26893 26897 26961 27027\n",
      " 27090 27127 27245 27250 27289 27310 27342 27495 27513 27558 27572 27618\n",
      " 27783 27867 27902 27996 28005 28068 28084 28141 28314 28318 28507 28541\n",
      " 28637 28678 28758 28839 28923 29027 29140 29349 29423 29448 29483 29510\n",
      " 29540 29660 29733 29773 29808 29943 29953 30035 30080 30202 30236 30282\n",
      " 30410 30424 30452 30457 30472 30691 30711 30831 31000 31007 31105 31203\n",
      " 31260 31263 31409 31533 31606 31671 31679 31686 32086 32107 32169 32198\n",
      " 32269 32277 32400 32401 32407 32435 32580 32610 32691 32734 32755 32907\n",
      " 32944 33072 33110 33120 33207 33208 33233 33272 33426 33679 33759 33773\n",
      " 33940 34118 34134 34208 34272 34292 34435 34446 34456 34457 34548 34559\n",
      " 34590 34601 34840 34888 34903 34951 34961 34991 35101 35110 35148 35208\n",
      " 35298 35358 35632 35651 35718 35853 35868 35886 35914 35934 36188 36385\n",
      " 36390 36483 36562 36774 36794 36813 36889 36893 36941 36943 37010 37012\n",
      " 37025 37029 37075 37220 37266 37293 37294 37304 37334 37336 37364 37397\n",
      " 37449 37498 37532 37623 37744 37824 37861 37927 37984 38266 38323 38395\n",
      " 38480 38489 38521 38550 38711 38968 38973 38997 39000 39013 39041 39077\n",
      " 39109 39239 39300 39318 39324 39339 39363 39379 39474 39480 39525 39557\n",
      " 39580 39646 39674 39695 39712 39714 39826 39838 39844 39898 40003 40119\n",
      " 40166 40222 40252 40382 40485 40516 40531 40556 40638 40753 40778 40814\n",
      " 40839 40947 40967 40993 41014 41054 41134 41194 41206 41275 41351 41611\n",
      " 41685 41782 41800 41813 41851 41908 41960 41964 42124 42158 42196 42242\n",
      " 42355 42407 42431 42455 42486 42498 42536 42649 42698 42703 42808 42902\n",
      " 42937 42939 43062 43162 43292 43351 43370 43452 43473 43585 43690 43702\n",
      " 43787 43818 43860 43864 43924 44016 44124 44139 44144 44198 44250 44304\n",
      " 44343 44408 44415 44436 44516 44523 44534 44558 44620 44637 44783 44829\n",
      " 44882 45044 45064 45095 45384 45421 45452 45463 45528 45663 45676 45698\n",
      " 45789 45872 45873 45949 45952 46026 46133 46134 46205 46424 46454 46540\n",
      " 46602 46658 46688 46708 46717 46831 46833 46885 46911 46992 47093 47149\n",
      " 47163 47277 47283 47358 47555 47695 47710 47729 47750 47849 47922 48072\n",
      " 48172 48181 48190 48282 48379 48454 48481 48613 48661 48739 48784 48797\n",
      " 48859 48976 49044 49077 49149 49236 49248 49326 49394 49402 49408 49423\n",
      " 49457 49494 49553 49584 49621 49632 49640 49653 49695 49710 49712 49807\n",
      " 49814 49826 49927 49978]\n",
      "________________________\n",
      "[  662  1215  2460  3038  6689  7929  9415 10354 11934 12401 13833 14979\n",
      " 15122 16600 17190 17958 18103 19368 20224 23137 24392 26281 29758 31227\n",
      " 31468 31730 32082 33962 34345 34833 34862 35691 36337 39004 41323 45321\n",
      " 46035 46840]\n",
      "________________________\n",
      "[ 2611  4633  8227 21737 22538 22782 30349 37338 38104 38912 42261 44003]\n",
      "________________________\n",
      "[   11    36   113 ..., 49961 49977 49986]\n",
      "________________________\n",
      "[   14    23    38 ..., 49939 49953 49974]\n"
     ]
    }
   ],
   "source": [
    "#seperate train data for k clusters \n",
    "\n",
    "datasets=[];\n",
    "for i in range(0,no_clusters):    \n",
    "    searchval = i\n",
    "    ii = np.where(data_cluster_list == i)[0]\n",
    "    print('________________________')\n",
    "    print(ii)\n",
    "    datasets.append(train_x[ii, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(949, 41)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(datasets))\n",
    "print(np.shape(datasets[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "      max_iter=-1, nu=0.08, random_state=None, shrinking=True, tol=0.001,\n",
      "      verbose=False)\n"
     ]
    }
   ],
   "source": [
    "OCC_list = list()\n",
    "\n",
    "#create one class SVM for each data cluster\n",
    "for i in range(no_clusters):\n",
    "    OCC_list.append(svm.OneClassSVM(nu=0.08, kernel=\"rbf\", gamma=0.1))\n",
    "    \n",
    "for i in range(no_clusters):\n",
    "    OCC_list[i].fit(datasets[i])\n",
    "    \n",
    "print(OCC_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 50.076\n"
     ]
    }
   ],
   "source": [
    "data_set=train_x\n",
    "dim=np.shape(data_set)\n",
    "error_count=0;\n",
    "for i in range(dim[0]):\n",
    "    bmu_index, bmu, sorted_list=som.find_bmu_list(data_set[i,:])\n",
    "    n=[data_cluster_list[i] for i in (sorted_list[0:30])]\n",
    "    classfier_list=np.unique(n)\n",
    "    #print(classfier_list)\n",
    "    flag=1\n",
    "    for classifier_id in (classfier_list):\n",
    "        #print(classifier_id)\n",
    "        y_pred_train = OCC_list[int(classifier_id)].predict(data_set[i,:].reshape(1, -1))\n",
    "        n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "        if n_error_train==0:\n",
    "            flag=0\n",
    "            #print('correct')\n",
    "    \n",
    "    if flag==1:\n",
    "        error_count=error_count+1\n",
    "        #print('error')\n",
    "    #print('---------------------')\n",
    "    \n",
    "print('error',(error_count*100/dim[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(dim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "10 57\n",
    "20 56\n",
    "30 55.668"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
